from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import requests
import time

driver = webdriver.Chrome() # Instantiate a Chrome driver
driver.get("https://example.com") # Navigate to the website

# Get the total number of pages in the pagination
def get_total_pages():
    pagination = driver.find_element_by_xpath("//ul[@class='pagination']")
    page_links = pagination.find_elements_by_tag_name("a")
    last_page_link = page_links[-2] # The second to last link is the link to the last page
    last_page = int(last_page_link.text)
    return last_page

total_pages = get_total_pages() # Get the total number of pages in the pagination
total_length = total_pages * 10 # The total length of the pagination is the total number of pages times 10

# Click on the "Next" button to go to the next set of pages
def click_next_button():
    next_button = driver.find_element_by_xpath("//a[contains(text(),'Next')]")
    next_button.click()

# Scrape images from the current page
def scrape_images():
    soup = BeautifulSoup(driver.page_source, "html.parser") # Create a BeautifulSoup object
    # Locate the image elements and extract the URLs
    image_elements = soup.find_all("img")
    image_urls = [img["src"] for img in image_elements]
    # Download the images
    for url in image_urls:
        response = requests.get(url)
        with open(url.split("/")[-1], "wb") as f:
            f.write(response.content)

# Loop through all the pages on the website
for i in range(1, 101):
    scrape_images() # Scrape images from the current page
    if i % 10 == 0: # If we have scraped 10 pages, click on the "Next" button to go to the next set of pages
        click_next_button()
        time.sleep(5) # Wait for the page to load before scraping images again

driver.quit() # Close the driver
